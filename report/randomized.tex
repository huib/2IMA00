The randomized algorithm as described in the book has been implemented as is described in the book. The original algorithm did only work for a given k. Therefore, to find the smallest k, the algorithm just iterates over all $k$ until it finds a suitable solution. Since it is a randomized algorithm it must have a provable error probability of at most $10^{-12}$ to qualify for the challenge. Since the original algorithm had an error probability of at most $1/e$, the algorithm does 28 times as much runs as described in the book since $(1/e)^x < 10^{-12}$ for $x \geq 28$.

\subsection{Randomized density algorithm}
Another randomized algorithm is implemented that is very similar to the normal randomized algorithm. To get an idea about what this algorithm does, and why it is better than the normal randomized algorithm, let $L = \langle v_1, v_2, ... , v_i\rangle$ be a sequence of vertices that are removed in one run of the algorithm, where vi is the vertex randomly removed at the ith call of the recursive function. In total $4^k$ such sequences are generated. Since the amount of vertices is much smaller than the amount of generated sequences, there are a lot of sequences that partly overlap with another sequence. For example take the sequence $\langle 1, 2, 4 \rangle$ and the sequence $\langle 1, 2, 3 \rangle$. In the normal algorithm both sequences are generated from the full graph, which means that kernelization is called in total 6 times. If we are smart though, we can calculate both sequences with 4 calls to the kernelization by first computing the sequence $\langle 1,2 \rangle$ and then calculating both sequences from this graph.

To do this the recursive method gets another parameter which denotes the amount of sequences that still have to be generated from the input graph. So the first call has the full graph and $28 * 4^k$ runs as input. In the method itself rather than removing one vertex, the amount of runs is divided over all the vertices in the kernelized graph using a distribution that reflects the result from taking a random vertex runs amount of time. Then the function is called recursively with the kernelized graph without the vertex for all of the vertices, and the amount of runs that were assigned to that subsequence. In other words, rather than distributing one run of the algorithm, all the runs are distributed according to the generated density. Hence the algorithm is dubbed the randomized density algorithm.

To amount of runs needs to be distributed over the possible subsequences resulting from deleting a vertex in a way that reflects the distribution made by the normal randomized algorithm. To do this the runs are first distributed over the edges. This is done by iterating over the edges, and assign an amount of r runs where r is binomial distributed with the amount of trials equal to the amount of not yet distributed runs, and the success probability equal to 1 divided by the amount of edges that still need runs distributed. (This is another way to simulate the distribution of throwing a die with one side for each edge). Then the edge itself distributes the runs assigned to it over its edges using the same formula (i.e. simulating throwing a coin for the assigned amount of runs).,

\subsection{Problems}
The algorithms above have two problems. The first problem is that the lower bound of the algorithm is equal to the upper bound of the algorithm, since in order to verify that $k$ is the lowest possible size vertex set, it first needs to verify that there is no vertex set with size $k-1$. This can only be done by running the problem at least $28*4^{k -1}$ times. A smarter algorithm might see that such a vertex set is not possible a lot faster. Therefore the algorithm does not work as fast as some other algorithms.

The second problem is related to the randomized density algorithm. Mathematically the distribution of the runs is sound in the sense that it does not affect the probability that the algorithm gives an incorrect answer. However, in the real world this is not possible. The algorithm relies on the discrete binomial distribution. However, it is not possible to generate a random variable from this distribution. To solve this a Guassian distribution is used, with a Z transformation and continuity reflection correction to approximate the binomial distribution. However, since it is an approximation this affects the probability that our algorithm gives an incorrect answer in such a way that we can not guarantee the necessary bound anymore. 