We implemented an algorithm based on iterative compression. The idea is that we start with an empty graph adding one
vertex to the graph every iteration until we reach the original graph again. Let $v_i$ denote the $i$th vertex, based on
some arbitrary ordering. For iteration $i$, let $G_i$ be the graph induced by the first $i$ vertices of the original graph
$G$. In the first iteration we have an empty graph $G_0$ and a trivially correct and optimal FVS $C_0 = \emptyset$. In
each subsequent iteration $i>0$ we find a FVS $C_i$ for the intermediate graph $G_i$, by first obtaining the FVS $C_{i-1}
\cup \{v_i\}$. When this FVS is larger than some given integer $k$, we apply a compression algorithm on the intermediate
FVS $C_{i-1} \cup \{v_i\}$ to find a smaller solution, or conclude that no smaller solution is possible. This algorithm
finds a FVS of size at most $k$, or concludes that no such solution exists. To find an optimal solution, this $k$ should
be equal to the size of the optimal solution. This problem is solved in section~\ref{sec:noK}.

In every iteration $i$, the compression algorithm is executed in order to find a FVS for a graph $G_i$ that is smaller in
size than a given FVS $X_i = C_{i-1} \cup \{v_i\}$. It achieves this by iterating over every non-empty subset $Z$ of
$X_i$. It labels all vertices in $Z$ as `prohibited', meaning we attempt to find a FVS of a small enough size that does
not include any vertex from $Z$, but does include every other vertex in $X_i$. Since $Z$ is a FVS for $G_i-(X_i\setminus
Z)$, we can apply the {\sc DisjointFVS} algorithm from the book ``Paramaterized algorithms'' \cite{ftpbook}, section 4.3.1
to find such a FVS or conclude that no such FVS exists.

\subsection{Finding the optimal solution without knowing its size} \label{sec:noK}
Since we don't know the size of the optimal solution we have to find way to run the algorithm without knowledge of $k$. We
use the compression algorithm every iteration to maintain an optimal solution for the intermediate graphs. This way we can
guarantee that the final solution is optimal. We know that the size of an optimal FVS for $G_i$ is at most one larger than
the size of an optimal solution for $G_{i-1}$, since only one vertex was added to the graph. There are two things that can
happen, the compression fails, proving there is no solution smaller than $C_{i-1} \cup \{v_i\}$, or the compression
succeeds, resulting in a solution of size $|C_{i-1} \cup \{v_i\}|-1 = |C_{i-1}|$, because no smaller FVS can exist. We
conclude that $C_i$ is optimal for $G_i$ for all iterations $i$. Also observe that $|C_{i-1}| \leq |C_i| \leq |C_{i-1}|
+1$.

\subsection{Implementation details}
The implementation of this algorithm is straightforward in most cases. We made one design decision that could be
significant to the performance. We implemented a simple way to revert changes made to the graph. Initially this was aimed
towards reverting the changes the reduction rules made while solving the {\sc DisjointFVS} problem, but since early tests
showed only little gain in time compared to cloning the graph, we only used two types of revertable actions: removing a
single vertex, and removing a set of vertices. The first action is used in order to add the vertices one by one, in the
reverse order we first removed them one by one. The second action was used before every time we ran the {\sc DisjointFVS}
algorithm in order to remove part of the to be compressed FVS from the graph and later revert this change.

\subsection{Improvements}
Although running the compression algorithm every iteration does not change the worst case
running time of the algorithm, running the compression algorithm every iteration might not seem a good idea for the
running time. However, it turns out that because we are maintaining an optimal solution for every intermediate graph we
can speed up the process, compensating the many times we run the compression algorithm.

\subsubsection{Reuse old solution when possible}
We have observed that $|C_{i-1}| \leq |C_i| \leq |C_{i-1}|+1$. An easy
improvement is to check whether $C_{i-1}$ is a FVS of $G_i$. If this is the case, we know that $C_{i-1}$ is an optimal
solution for $G_i$ and assign $C_i = C_{i-1}$, skipping the compression algorithm for this iteration.

\subsubsection{Skip half the work}
For another improvement, observe that when in the compression algorithm, a non-empty
subset $Z$ of $X_i$ does not include $v_i$, meaning $v_i \in X_i\setminus Z_i$, the {\sc DisjointFVS} algorithm is run
on graph $G_i-(X_i\setminus Z)$ with prohibited vertices $Z$. Observe that $G_i-(X_i\setminus Z) =
G_{i-1}-(X_{i-1}\setminus Z)$ when $v_i \not\in Z$. If {\sc DisjointFVS} would return that there exists a FVS $C$ for
$G_i-(X_i\setminus Z)$ with $|C|<|Z|$, then this $C$ is also a FVS for $G_{i-1}-(X_{i-1}\setminus Z)$, meaning $C \cup
(X_{i-1}\setminus Z)$ is a FVS for $G_{i-1}$ of size $|C \cup (X_{i-1}\setminus Z)| < |X_{i-1}|$. Since there cannot be
a FVS for $G_{i-1}$ smaller than $X_{i-1}$, this is a contradiction, hence {\sc DisjointFVS} will return that there is
no solution when $v_i \not\in Z$. This means we can skip the half of all non-empty subsets of $X_i$ that do not include
$v_i$, speeding up the algorithm by almost a factor $2$. To be able to apply this strategy, the compression algorithm
needs to be told which vertex has been added since the last compression.

Note that this strategy can also be applied when we are not applying the compression every iteration, but we would have
to consider a set of multiple vertices added since the last compression, instead of only a single one. This means only a
very small part of the subsets of $X_i$ (those containing all the vertices added since the last compression) can be
skipped.

%\subsubsection{Check for cycles only at the start of the recursion}
%The {\sc DisjointFVS} algorithm is recursive. Before
%any reduction rules are applied to the graph, the algorithm checks for cycles in the set of prohibited vertices. If
%there is a cycle, then there cannot be a FVS that does not contain any of the prohibited vertices. This check is only
%required in the very first layer of the recursion. No cycles will be added when going deeper in the recursion. New
%vertices can be labelled a `prohibited', but these never introduce cycles. Checking only at the start of the recursion
%saved some time.

\subsubsection{Ordering the vertices} \label{sec:order}
Because we can skip the costly compression algorithm when the old solution is
still valid in the new graph, the order in which we add the new vertices makes a large difference. Since the compression
algorithm is triggered only when the new vertex introduces a cycle that does not include a vertex in the old solution,
we want an order such that the number of vertices that introduce such a cycle is low. This becomes exponentially more important when $k$ grows. One way of looking at this is that we must try to keep $k$ low for as long as possible, reducing the number of compressions with a higher $k$. But it may also be possible to pick the vertices triggering a compression in such a way that, when the compression fails and $k$ grows, the new solution (containing the last vertex we added) allows a large amount of vertices to be added afterwards with a minimal chance of needing to compress. In other words, try to make the intermediate solutions `future proof'.

Because of the way we implemented adding the next vertex by reverting a deletion, we were forced to decide upon a vertex
order at the start of the algorithm and could not easily change the order on the go. We have experimented with a number of
orderings, based on their degree in $G$ and their weight given by the approximation algorithm described in 
section~\ref{sec:2approx}. For degree and weight we tried increasing and decreasing order and tried
all 8 lexicographic orderings on degree and weight. Out of these 12 ordering, the lexicographic ordering on increasing
degree then increasing weight was fastest.

We also tried to use a 2-approximation solution to skip a large number of vertices. Set set of vertices no in this
2-approximation form a forest by definition, hence we can safely add all these vertices without compression. Now we have
at most $2k$ iterations left in which we may need to compress. Ordering these last $2k$ vertices using the same
lexicographic ordering on increasing degree then weight resulted in no significant gain in speed, but did reduce the standard deviation of running times, as one would expect with less permutations. Although this method does not significantly improve the speed, it does prevent slow runs, so we chose to leave it in.

\subsection{Future improvements}
\subsubsection{Implement a faster disjoint algorithm}
The book ``Paramaterized Algorithms'' \cite{ftpbook} describes a faster algorithms solving {\sc DisjointFVS} using a
fourth reduction rule, resulting in an algorithm for FVS that runs in $O(3.6181^kn^{O(1)})$ time. Unfortunately the implementation of this algorithm was too complex for the time available, and it is unclear whether this algorithm will be faster in practice.

\subsubsection{Reorder vertices on the go}
As you may have thought reading section~\ref{sec:order}, it may be good to choose a next vertex to add after the
intermediate solution is calculated. This way we may be able to better limit the number of compressions required. Because
one of our initial design decisions prevented us from efficiently redefining the order, so we where unable to try this
method.

\subsubsection{Use different orderings in parallel}
We may be able to compare multiple runs of the iterative compression algorithm before the runs have finished, determining which runs seem to have a particular bad ordering by chance. We can interleave the iterations of multiple runs of the same algorithm, each using a different ordering, and kill the ones that show least promise to finish quickly. Although we do not think thing will result in a faster running time, it may still be interesting to try.

\subsubsection{Subset iteration}
In our current implementation, the order in which we iterator over all possible subsets
of the to be compressed solution is fixed in such a way that for every next subset, only one vertex is added or removed.
Since performing more operations to obtain the next subset will not have a significant impact on the running time, they
may be a large advantage in changing the order in which we iterate over the subsets. Since we have access to information
from previous compressions, we may be able to estimate which subsets are more likely to be successful. Depending on how
good this estimation is, trying these subsets first can speed up the compression by a lot.
