%Let's not describe what each step does like this to save more space for other topics
%\todo{Add intro/explanation about there being 4 different kernelization options.}\\
% als korte inleiding voor de overige subsecties. Met en zonder k-dependency -> benoem ook gelijk hun time complexity
% The result of solving the problem on the kernel should either be the same as on the original input, since we ensure that we adjust the size $k$ of our solution accordingly whenever we remove a vertex that should be part of the solution
Preprocessing via data reduction or, more specifically, kernelization as a strategy of coping with hard problems is universally used in almost every implementation. The FVS problem is NP-hard and as is shown by Karp [ref1], its NP-complete for both directed and undirected graphs even if the graphs are unweighted. Due to this intractability, V. Bafna et al. suggest using a polynomial time algorithm for computing a near optimal FVS [ref2]. They show that their factor-2 approximation algorithm called \textsc{ FEEDBACK} is a simple and efficient approximation algorithm that can be implemented to run in $O(min\{|E| log |V |, |V |^2\})$ time, or $O(|V |^2)$ in our particular implementation. Therefore, in coherence with the kernelization reduction rules, an algorithm has been implemented based \textsc{ FEEDBACK} in an attempt to simplify and speed up the detection of solutions to the FVS problem. This approximation is preceded and followed by various reduction rules in the implementation that allow the onfollowing processes of the application to apply their methods to reduced problem instances. Results have shown that the application of the kernelization significantly improved the processing speed of the entire application and, consequently, increased the number of solutions found within an acceptable amount of time in combination with the other processes. \\\\
Since kernelization may be used in differing situations, we made sure to allow users access to two binary choices of flexibility. First of all, we do not always know a concrete value for $k$, the desired FVS size. If this is not known, we may skip tests that check whether an FVS for a given $k$ is still possible. For all other purposes, we can substitute $k$ by an approximation of $k$. Additionally, since some algorithms will want to kernelize very often and do very little in-between, we allow the user to skip the advanced rules. This allows a significant reduction in overhead, as our advanced rules are applied in $O(|V|^3)$ time, whereas the simple ones are done in $O(|V|^2)$ time, which in practice works out closer to $\Theta(|V|)$.\\\\
Note that the problem instances $(G,k)$ that are used are read as multigraphs, which means that they do not support self-loops as opposed to allowing multiple edges between any two vertices. Therefore, if the modification to $G$ caused by a reduction rule would theoretically induce a self-loop, the appropriate vertex inducing that self-loop along with its incident edges are immediately removed from the graph $G$ following the Self-Loop Rule, while adjusting parameter $k$ accordingly.

\subsection{Simple Kernelization} % uitleg basis kernelization en terugkoppeling naar kernelittle
Several basic reduction rules that each make simple changes to the graph have been implemented. The safeness of these reduction follows from Proposition 2 as described by H. Bodlander [ref3]. The simple kernelization process firstly removes isolated vertices $v$ from graph $G$, as well as leaves and their incident edges from $G$. Doing so does not change parameter $k$. Next, if a vertex $v$ has degree two and neighbors $a$ and $b$, possibly allowing $a=b$, then the simple kernelization modifies $G$ by replacing $v$ and its two incident edges with a single edge between its two neighbors $a$ and $b$. Note that if this were to create a self-loop, then the simple kernelization method also removes the vertex $a=b$ and its incident edges and reduces parameter $k$ accordingly. Whenever an edge is added, we make sure to reduce the multiplicity of that edge to $2$ if it is greater than $2$. \\\\
In order to apply these rules exhaustively, we have considered $3$ methods. The first method is to loop over all vertices repeatedly until a full cycle finishes without change. The second method is to maintain a queue with duplicates, adding neighbours of a changed vertex for re-consideration. Finally, we used a queue without duplicates, thus having to make sure not to add vertices to the queue that are still in it. From testing it turned out that the repeated looping and queue with duplicates run fastest, where the queue without duplicates runs a factor $3$ slower. This is likely due to the reduction rules taking less time than insertions in the queue without duplicates. We also suspect that the graph stabilizes very quickly when applying these rules.\\\\
In case a value for $k$ is passed on to the kernelization, the simple kernelization function also makes checks whether the graph can still accomodate an FVS for the given $k$. This is done firstly by checking that $k'$, $k$ after subtracting all self-loops found, is still at least $1$ for a non-empty graph after exhaustively applying the simple reductions. As a second rule, we check whether the number of vertices $|V(G)|$ in the graph $G$ satisfies $|V(G)|\leq k(d+1)$ where $d$ is the maximum degree found in $G$, when false, no FVS can be found for such $k$. Additionally, we check for the number of edges $|E(G)|$ in $G$ satisfies $|E(G)|\leq 2kd$, again, concluding no FVS of size $k$ is possible if false.
%\subsubsection*{Rule 0-1: The Degree-Zero and Degree-One Rules}
%Firstly, we process problem instances in our implementation by simultaneously performing both the Degree-Zero Rule and the Degree-One Rule. We do this by removing any unconnected vertices we find within a graph $G$. That is to say, if $v$ is an isolated vertex, meaning there is no edge incident to $v$, then remove $v$ from $G$. Furthermore, we also remove leafs from $G$ by removing every $v$ that has degree one along with its incident edge from graph $G$. Applying both of these reduction rules does not change our parameter $k$.
%\subsubsection*{Rule 2: The Degree-Two Rule}
%If a vertex $v$ in $G$ has 2, with neighbors $a$ and $b$ and while possibly allowing $a = b$, then we modify $G$ by replacing $v$ and its two incident edges with a single edge between $a$ and $b$. Unless this modification induces a self-loop, parameter $k$ remains unchanged after applying it. In case this modification would result in the creation of a self-loop, namely when $a=b$, then we immediatly remove it using Rule 3: The Self-Loop Rule and adjust parameter $k$ accordingly.
%\subsubsection*{Rule 3: The Self-Loop Rule}
%If there is a loop on a vertex $v$, then we take $v$ into the solution set and reduce our problem instance $(G,k)$ to the instance $(G - v, k - 1)$. Note that since we deal with multigraphs, our original problem instances $(G,k)$ do not support self-loops, because a multigraph is a non-simple undirected graph in which such loops are not permitted, but in which multiple edges between any two vertices are allowed. Therefore, in our implementation, we simply remove self-loops before they practically ocuur whenever applying any kernelization reduction rule would technically induce a self-loop.
%\subsubsection*{Rule 4: The Multiedge Reduction Rule}
%If there are more than two edges between $u$ and $v$, then we delete all but two of these edges, which does not change our parameter $k$.

\subsection{The 2-Approximation for the Undirected FVS Problem}%ref: http://epubs.siam.org/doi/abs/10.1137/S0895480196305124, karp is ref of V. Bafna et al
\label{sec:2approx}
% 1. Waarom approximation (staat in intro)
% 2. Is uitvoering binnen running time aangegeven door paper (+correctness -> we geven aan waarom die de paper accuraat volgt)
% 3.uitleg data structuren etc
% 4. (voor resultaten) verschil tussen resultaten met en zonder gebruik van approximation
% [point-2:]
Given a graph $(G, w)$ with $G = (V,E)$ and $w$ being the weighted vertices in $V$, any vertex of weight zero is removed from $G$ and placed in the solution set $F$ at the outset of the \textsc{Feedback} algorithm. \textsc{FEEDBACK} then decomposes $(G, w)$ into subgraphs $(G_i, w_i)$â€™s (in the first While loop) by iteratively subtracting $w_i$ from $w$, removing vertices of weight reduced to zero, adding them into $F$, and cleaning up $G$ by using the CleanUp procedure, which recursively deletes vertices of degree $\leq$ 1, until $G$ becomes empty.\\\\
The implementation of \textsc{Feedback} achieves the same result. First, applies the cleanUp helper function that removes all vertices with degree $\leq$ 1 to produce a clean graph where all remaining vertices have degree $\geq$ 2. Afterwards, it creates a HashMap from the remaining vertices to a default weight value of 1 that is consistently used to modify and track the weights of all vertices within the graph of interest. While the original graph $G$ still con %wordt 1 juli afgemaakt
\todo{TODO:remainder of approximation. Meaning/use of result}

\subsection{Advanced Kernelization}% uitleg geavanceerde kernelization stappen en terugkoppeling naar kernelot
%\todo{Remaining Reduction rules}
Simple kernelization rules have the advantage of giving the graph certain properties for algorithms to work with. Advanced kernelization steps are then taken in addition to remove a lot more of unnecessary information from the graph. Usually the aim is to end up with a provable bound to the kernel size in terms of $k$. We only managed to implement a single rule that aims to filter out vertices guaranteed to be in the optimal FVS, thus we have no bound on the resulting graph size. 
\subsubsection{Strongly Forced Vertex Rule}
The reduction rule we use, the Strongly Forced Vertex rule, is a reduction similar to the commonly seen Flower reduction. The Flower reduction finds a vertex that must be in the FVS due to being the single common vertex in more than $k$ cycles. This rule usually requires calculating a maximum matching, taking at least $O(|V|^2)$ time. \\\\
The Strongly Forced Vertex rule removes those same vertices in a different way, and possibly more, using only the approximation algorithm, which is at least as fast as maximum matching. Instead of finding a vertex with flower properties, we simply run the approximation algorithm on the weighted version of $G$, $G_w$. In $G_w$ all vertices have weight $1$, except the vertex $v$ we want to check with the rule, which has weight $2k+1$. If $v$ must have been in the FVS, then the approximation will either put it in, or put in a lot of other vertices to cover for it. When the approximate weighted FVS has a weight of at least $2k+1$, then $v$ must be in the FVS. In that case, $v$ is removed from the graph, added to the solution and $k$ is reduced by $1$. The correctness of this reduction follows from the fact that no FVS of weight at most $2k$ could be found when $v$ had increased weight. If $v$ was not required in the FVS, then the 2-approximation would return an FVS of lower weight.
\subsubsection{Failure to reduce}
Sadly, in all our test instances, this single reduction rule does not remove vertices from any of our test instances when using an approximated $k$. In these cases it makes sense, as a vertex would have to be extremely central to the problem to still be removed in the approximated case. Because of this disadvantage, applying this reduction for pre-processing is pointless. \\\\
We also tried using this reduction within algorithms, when a value for $k$ is given. However, the randomized algorithms and iterative compression algorithm, described later, are the only candidates for this. The problem is that these algorithms gain most of their processing time from the vast number of kernelization usages. Given the significant increase in running time of the advanced rules and the infrequency of its usefulness, this caused orders of magnitude of slow-down. The running time stems from having to apply the approximation algorithm once for each vertex. We did not bother to measure this exact slow down, as even smaller instances would already take too much time.
\subsubsection{Improvements}
In order to make this advanced kernelization more useful, we will have to complete a set of rules that works together to offer a bound on the resulting graph size. Most notably the rules referred to as Improvement and Abdigation. \\\\
The Improvement rule finds two vertices of which either must be in the FVS, and connects them with a double edge to make the same conclusion easier. This can be done through the approximation algorithm again, as the Strongly Forced Pair rule. For this rule we do the same as the Strongly Forced Vertex rule, but we increase the weight of two vertices rather than one. Since this rule primarily sets up for other advanced rules, notably Abdigation, to play off of, we did not include it in the system. Aside from the lack of use, we would have to check all pairs, adding another factor $|V|$ to the running time of the advanced rules.\\\\
Together with Abdigation, which we refrain from describing, this advanced kernelization would supposedly lead to a graph with $O(k^3)$ vertices. Given even just an approximate of $k$, this would provide a significant decrease in problem size after pre-processing.

\subsection{Kernelization Running Time} %kernelot tijd, kernelittle tijd
\todo{Why the choice between kernelot and kernelittle matters}
% Opsomming looptijd van voorheenbesproken onderdelen
% Let uit waarom de keuze tussen kernelot en kernelittle er toe doet
% Let uit waarom, ondanks de nadelen van het gebruik van de extras die kernelot aanbied, kernelot gewenster kan zijn dan kernelittle en vice versa

