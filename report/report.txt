\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}
\title{Report Feedback Vertex Set}

\author{Stefan Majoor (08059008) \\
Huib Donkers\\
Henk Alkema \\
Xi ... \\
Leo ... \\
Christopher Ankomah\\}

\date{\today}
\begin{document}
\maketitle
\newpage

\noindent \textbf{Abstract} \\
This report gives an overview of our solution for the track B of The 1st Parameterized Algorithms and Computational Experiments Challenge. In this challenge the feedback vertex set problem is solved. In total three different algorithms were implemented: the standard randomized algorithm, the iterative compression algorithm and a solution based on tree composition. The solution based up on the tree composition was too slow, and did not yield any meaningful results. The randomized algorithm worked fine for instances with a relatively low k.  For larger k the iterative compression gave the best solution. Still the algorithm was only able to solve a small subset of the available problems. 

\section{Kernelization}

\section{SplitSolve}
To make the algorithms faster the splitsolve algorithm is implemented.The splitsolve algorithm makes use of the divide and conquer algorithm. It first takes a graph, and removes all edges not part of a cycle, since they do not affect the outcome for the end result. This is done using a dfs from each vertex to itself. Then it detects connected components of the graph. Since the minimal feedback vertex set is equal to the union of the minimal vertex set of the connected components of a graph, the algorithm then calls one of the other algorithms for each of the new graphs. Especially for the randomized algorithm this makes a huge difference in running time, since the new running time is now bounded by the solution size of the largest possible connected component, and not for the solution size for the whole graph anymore. 

\section{Randomized Algorithm}
The randomized algorithm as described in the book has been implemented as is described in the book. The original algorithm did only work for a given k. Therefore, to find the smallest k, the algorithm just iterates over all k until it finds a suitable solution. Since it is a randomized algorithm it must have a provable error probability of at most $10^-12$ to qualify for the challenge. Since the original algorithm had an error probability of at most 1/e, the algorithm does 28 times as much runs as described in the book since $(1/e)^x < 10^-12$ for $x \geq 28$.

~\\\\\noindent\textbf{Randomized density algorithm}\\
Another randomized algorithm is implemented that is very similar to the normal randomized algorithm. To get an idea about what this algorithm does, and why it is better than the normal randomized algorithm, let L = <v1, v2, … , vi> be a sequence of vertices that are removed in one run of the algorithm, where vi is the vertex randomly removed at the ith call of the recursive function. In total $4^k$ such sequences are generated. Since the amount of vertices is much smaller than the amount of generated sequences, there are a lot of sequences that partly overlap with another sequence. For example take the sequence <1, 2, 4> and the sequence <1, 2, 3>. In the normal algorithm both sequences are generated from the full graph, which means that kernelization is called in total 6 times. If we are smart though, we can calculate both sequences with 4 calls to the kernelization by first computing the sequence <1,2> and then calculating both sequences from this graph.

To do this the recursive method gets another parameter which denotes the amount of sequences that still have to be generated from the input graph. So the first call has the full graph and $28 * 4^k$ runs as input. In the method itself rather than removing one vertex, the amount of runs is divided over all the vertices in the kernelized graph using a distribution that reflects the result from taking a random vertex runs amount of time. Then the function is called recursively with the kernelized graph without the vertex for all of the vertices, and the amount of runs that were assigned to that subsequence. In other words, rather than distributing one run of the algorithm, all the runs are distributed according to the generated density. Hence the algorithm is dubbed the randomized density algorithm.

To amount of runs needs to be distributed over the possible subsequences resulting from deleting a vertex in a way that reflects the distribution made by the normal randomized algorithm. To do this the runs are first distributed over the edges. This is done by iterating over the edges, and assign an amount of r runs where r is binomial distributed with the amount of trials equal to the amount of not yet distributed runs, and the success probability equal to 1 divided by the amount of edges that still need runs distributed. (This is another way to simulate the distribution of throwing a die with one side for each edge). Then the edge itself distributes the runs assigned to it over its edges using the same formula (i.e. simulating throwing a coin for the assigned amount of runs).,


~\\\textbf{Problems}\\
The algorithms above have two problems. The first problem is that the lower bound of the algorithm is equal to the upper bound of the algorithm, since in order to verify that k is the lowest possible size vertex set, it first needs to verify that there is no vertex set with size k-1. This can only be done by running the problem at least $28*4^(k -1)$ times. A smarter algorithm might see that such a vertex set is not possible a lot faster. Therefore the algorithm does not work as fast as some other algorithms.

The second problem is related to the randomized density algorithm. Mathematically the distribution of the runs is sound in the sense that it does not affect the probability that the algorithm gives an incorrect answer. However, in the real world this is not possible. The algorithm relies on the discrete binomial distribution. However, it is not possible to generate a random variable from this distribution. To solve this a Guassian distribution is used, with a Z transformation and continuity reflection correction to approximate the binomial distribution. However, since it is an approximation this affects the probability that our algorithm gives an incorrect answer in such a way that we can not guarantee the necessary bound anymore. 


\section{Iterative Compression}
To test the different algorithms 7 different graphs were determined that were relatively easy to solve. The reason that relatively easy graphs were taken was because not all algorithms gave an answer for the larger graphs. The chosen graphs were then solved 10 times by each algorithm, and the average solving time was taken to compare the algorithms. 

~\\\textbf{Running time full algorithm} \\
The following table gives an overview of the time it took each algorithm to solve the problem in their final form.

~\\\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Problem} & \textbf{k} & \textbf{Randomized} & \textbf{Randomized Density} & \textbf{Iterative Compression} \\
\hline
096 & 6 & 695 ms & 107 ms & 40 ms \\
099 & 8 & 743 ms & 168 ms & 6 ms \\
050 & 7 & 4.407 ms & 8.515 ms & 37 ms \\
062 & 7 & 3.857 ms & 1.591 ms & 39 ms \\
083 & 7 & 4.897 ms & 12.798 ms & 12 ms \\
095 & 8 & 17.159 ms & 8.851 ms& 40 ms \\
028 & 8 & 18.124 ms & 9.994 ms& 40 ms \\
\hline
\textbf{Total:} & & 49.882 ms & 40.433 ms & 214 ms\\
\hline
\end{tabular}

~\\\\\textbf{Split Algorithm} \\
To show that the split algorithm works the iterative compression is run on all the instances that it could solve within 20 seconds. Then the iterative compression was run without doing splitsolve first. These were the results:

~\\\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Problem} & \textbf{k} & \textbf{Without Split Solve} & \textbf{With Split Solve} & \textbf{Change} \\
\hline
003 & 10 & 396 ms & 401 ms  & +1,2\% \\
006 & 11 & 2827 ms & 1.127 ms & -60.13\% \\
020 & 8 & 35 ms & 27 ms &  -35.71\% \\
028 & 8  & 47ms & 42 ms & -10.64 \% \\
031 & 33 & $>$ 5 min & 153 ms & $>$ -99.95\% \\
042 & 11 & 246 ms & 142 ms & -42.28 \% \\
050 & 7 & 12 ms & 14 ms & 16.67\% \\
072 & 9 & 521 ms & 509 ms & -2.3\% \\
083 & 7 & 9 ms & 10 ms & 11.11\% \\
085 & 51 & $>$ 5 min & 34 ms & $>$ -99.98\% \\
091 & 21 & $>$ 5 min & 2.037 ms & $>$ - 99.32\% \\
095 & 8 & 33 ms & 38 ms & 15.15\% \\
096 & 6 & 9 ms & 11 ms & 22.22\% \\
099 & 8 & 17 ms & 6 ms & -64.71\% \\
\hline
\end{tabular}




\section{Tree width problem}

\section{Test Results}

\section{Conclusion}


\end{document}
